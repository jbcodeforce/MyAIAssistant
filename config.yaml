# Default configuration for MyAIAssistant Backend
# These values are used unless overridden by:
# 1. User config file (set CONFIG_FILE environment variable)
# 2. Environment variables
# 3. .env file

# Database settings
# SQLite (default for local deployments):
database_url: "sqlite+aiosqlite:///./data/app.db"
# PostgreSQL (for docker-compose.postgres.yml or production):
# database_url: "postgresql+asyncpg://postgres:postgres@postgres:5432/myaiassistant"

# CORS settings (list of allowed origins)
cors_origins:
  - "http://localhost:5173"
  - "http://localhost:3000"

# LLM settings
# Provider must be "huggingface" - the InferenceClient connects to Ollama via base_url
# Ollama's OpenAI-compatible endpoint is at /v1
llm_provider: "huggingface"
llm_model: "gpt-oss:20b"
llm_api_key: null
llm_base_url: "http://localhost:11434/v1"
llm_max_tokens: 2048
llm_temperature: 0.1
